# 第九章 并发编程

## 9.1 操作系统基础

- 操作系统发展史
- 并发和并行
- 阻塞和非阻塞
- 同步和异步
- 进程：三状态图，唯一标示，开始和结束
- 线程

### 1. 操作系统发展史

#### 1.1 人机矛盾(cpu利用率低)

—>磁带存储+批处理(降低数据的读取时间,提高cpu的利用率)

—>**多道操作系统**：数据隔离、时空复用(能够遇到**I/O**操作的时候主动把cpu让出来，给其他任务使用，切换需要时间，由OS完成)

—> 短作业优先算法、先来先服务算法

—>**分时OS**：时间分片，CPU轮转，每一个程序分配一个时间片，**降低了cpu利用率**，**提高了用户体验**

—>**分时**OS + **多道**OS：多个程序一起执行，遇到IO切换，时间片到了也要切换

- **多道技术介绍**

1. 产生背景：针对单核，实现并发
2. 现在的主机一般是多核，那么每个核都会利用多道技术有4个cpu，运行于cpu1的某个程序遇到io阻塞，会等到io结束再重新调度，会被调度到4个 cpu中的任意一个，具体由操作系统调度算法决定。
3. 空间上的复用：如内存中同时有多道程序
4. 时间上的复用：复用一个cpu的时间片

**Note**：遇到io切，占用cpu时间过长也切，**核心**在于切之前将进程的状态保存下来，这样
才能保证下次切换回来时，能基于上次切走的位置继续运行。  

#### 2.2 操作系统**分类**(4)

**OS作用**：将应用程序对硬件资源的**竞态请求**变得有序化

- **实时OS**：实时控制，实时信息处理
- **通用OS**：多道、分时、实时处理或其两种以上
- **网络OS**：自带网络相关服务
- **分布式OS**：python中可使用：**celery**模块

### 2. 进程

#### 2.1 进程：运行中的程序

- 程序只是一个文件，进程是程序文件被cpu运行
- 进程是计算机中最小的**资源分配**单位
- 在OS中有唯一标示符**PID**

#### 2.2 OS调度算法(4)

- 短作业优先
- 先来先服务
- 时间片轮转
- **多级反馈算法**：分时+先来先服务+短作业优先

#### 2.3 并行与并发

1. **并发**（concurrency）：把任务在**不同的时间点**交给处理器进行处理。看起来程序同时执行，实际在同一时间点，任务并不会同时运行。
2. **并行**（parallelism）：把每一个任务分配给每一个处理器独立完成。在同一时间点，任务一定是同时运行。
3. **并发的本质**：切换+保存状态

### 3. 同步异步阻塞非阻塞

1. **同步**：调用一个函数/方法，需要**等待**这个函数/方法**结束**
   - **一个功能调用时，没有得到结果之前，就不会返回，可以说是一种操作方式。**
2. **异步**：程序同时运行，没有**依赖**和**等待**关系，调用一个方法，**不等待**这个方法**结束**，不关心这个方法做了什么
3. **阻塞**：cpu不工作
   - **阻塞调用**是指调用结果返回之前，**当前线程**会被挂起。函数只有在得到结果之后才会返回
4. **非阻塞**：cpu工作
   - 调用指在不能立刻得到结果之前，该**调用不会阻塞当前线程**。
5. **同步阻塞**
   - con.recv()，socket阻塞的tcp协议
6. **同步非阻塞**
   - 没有io操作的func()
   - socket非阻塞tcp协议； 调用自定义函数(不存在io操作)
7. **异步非阻塞**（重点）
   - 没有io操作，把func扔到其他任务里各自执行，cpu一直工作
8. **异步阻塞**
   - 程序中出现io操作

#### Note1(2)

1. 同步和异步关注的是**消息通信机制** (synchronous communication/ asynchronous communication)
2. 阻塞和非阻塞关注的是**程序在等待调用结果（消息，返回值）时的状态**

### 4. 进程的三状态图

#### 1. 进程状态

**进程状态**：运行(runing)  就绪(ready)  阻塞(blocking)

![进程三状态图](/Users/henry/Documents/%E6%88%AA%E5%9B%BE/Py%E6%88%AA%E5%9B%BE/%E8%BF%9B%E7%A8%8B%E4%B8%89%E7%8A%B6%E6%80%81%E5%9B%BE.png)

#### 2. 进程的创建与结束

- **进程创建**

1. 系统初始化(**ps**)
2. 一个进程开启了一个子进程(os.fork，subprocess.Popen)
3. 用户交互式请求(用户双击app)
4. 批处理作业的初始化(只在大型机的批处理系统中应用)

- **进程结束**

1. 正常退出
2. 出错退出
3. 严重错误
4. 被其他进程杀死(**kill -9 pid**)



## 9.2 进程

### 1. 进程与线程

#### 1.1 分别做多件事

- 如果是两个程序分别做两件事
  - 起两个进程
- 如果是一个程序，要分别做两件事
  - 起线程，视频软件：下载电影1，电影2，电影3

#### 1.2 进程解析

1. **进程**（**Process**）是计算机中的程序关于**某数据集合上**的一次运行活动，是系统进行**资源分配和调度**的**基本单位**，是OS结构的基础。

    

2. 在早期面向进程设计的计算机结构中，进程是程序的基本执行实体；在当代**面向线程设计**的计算机结构中，**进程是线程的容器**。程序是指令、数据及其组织形式的描述，**进程是程序的实体。**

3. 顾名思义，进程即正在执行的一个过程。**进程**是对正在运行程序的一个抽象。

4. 进程的概念起源于操作系统，是操作系统最核心的概念，也是操作系统提供的最古老也是最重要的**抽象概念**之一。操作系统的其他所有内容都是围绕进程的概念展开的。

   **PS**：即使可以利用的cpu只有一个（早期的计算机确实如此），也能保证支持（伪）并发的能力。将一个单独的cpu变成多个虚拟的cpu（多道技术：时间多路复用和空间多路复用+硬件上支持隔离），没有进程的抽象，现代计算机将不复存在。

5. **进程概念**

   - 进程是一个程序实体。每**一个进程**都有它自己的**地址空间**，一般情况下，包括**文本区域**（text region）、**数据区域**（data region）和**堆栈**（stack region）。文本区域存储处理器执行的代码；数据区域存储变量和进程执行期间使用的**动态分配**的内存；堆栈区域存储着活动过程调用的**指令和本地变量**。
   - 进程是一个“执行中的程序”。程序是一个没有生命的实体，只有处理器赋予程序生命时（操作系统执行之），它才能成为一个活动的实体，我们称其为进程。
   - 进程是操作系统中最基本、重要的概念。是多道程序系统出现后，为了刻画系统内部出现的动态情况，描述系统内部各道程序的活动规律引进的一个概念,所有**多道程序设计操作系统**都建立在**进程**的基础上。

6. **特点**

   - 是计算机中最小的**资源分配单位**，**数据隔离**。
   - 创建、销毁、切换进程时间**开销大**
   - 可以利用多核

#### 1.3 线程(5)

1. 线程是进程中的一部分，不能脱离进程存在
2. 任何进程中至少有一个线程，**只**负责执行代码，不负责存储共享的数据，也不负责资源分配
3. 创建、销毁和切换的开销**远远小于**进程
4. **线程**是计算机中能被**cpu调度的最小单位**
   - 爬虫使用需要配合前端
5. 一个进程中的多个线程可以共享这个进程的数据——  **数据共享**

#### 1.4 开销

1. 线程的创建和销毁
   - 需要一些开销(一个**存储局部变量**的数据结构，**记录状态**)
   - 创建、销毁、切换**开销**远**远小于**进程
2. python中的线程比较特殊，所以进程也有可能被用到
3. **进程**：数据隔离、开销大、数据不安全、可以利用多核、os切换
4. **线程**：数据共享、开销小 、数据不安全、不可以利用多核、os切换

### 2. 进程的创建

- 仔细说来，**multiprocess**不是一个模块而是python中一个操作、管理进程的包。 之所以叫multi是取自multiple的多功能的意思,在这个包中几乎包含了和进程有关的所有子模块。由于提供的子模块非常多，为了方便大家归类记忆，我将这部分大致分为四个部分：**创建进程**部分，**进程同步**部分，**进程池**部分，进程之间**数据共享**。

#### 2.1 multiprocessing

- 基于process模块

```python
# 获取进程的pid, 父进程的id及ppid
import os
import time
print('start')
time.sleep(20)
print(os.getpid(),os.getppid(),'end')
```

#### 2.2 子进程和父进程

1. pycharm中启动的所有py程序都是pycharm的子进程

```python
# 把func函数交给子进程执行
import os
import time
from multiprocessing import Process

def func():
  print('start', os.getpid())
  time.sleep(1)
  print('end', os.getpid())

if __name__ == '__main__':	  
  p = Process(target=func)				      # 创建一个即将要执行的进程对象
  p.start()	                            # 开启一个进程，异步非阻塞
  p.join()												      # 同步阻塞，直到子进程执行完毕
  print('main', os.getpid())		      	# 异步的程序，调用开启进程的方法，并不等待这个进程的开启
```

2. **创建子进程注意**

ps：_\_name__ 只有两种情况，**文件名**或**双下划线**main字符串

```python
# windows
通过（模块导入）执行父进程文件中的代码获取父进程中的变量
只要是不希望被子进程执行的代码，就写在if __name__ == '__mian__'下
进入导入时，父进程文件中的 __name__ != '__mian__'
# linux/macos
创建新的子进程是copy父进程内存空间，完成数据导入工作（fork）,正常写就可以

公司开发环境都是linux，无需考虑win中的缺陷
# windows中相当于把主进程中的文件又从头执行了一遍

# linux，macos不执行代码，直接执行调用的函数在Windows操作系统中由于没有fork(linux操作系统中创建进程的机制)，在创建子进程的时候会自动 import 启动它的这个文件，而在 import 的时候又执行了整个文件。因此如果将process() 直接写在文件中就会无限递归创建子进程报错。所以必须把创建子进程的部分使用if __name__ ==‘__main__’ 判断保护起来，import 的时候  ，就不会递归运行了。
```

- 父进程(主进程)存活周期

![父子进程](/Users/henry/Documents/%E6%88%AA%E5%9B%BE/Py%E6%88%AA%E5%9B%BE/%E7%88%B6%E5%AD%90%E8%BF%9B%E7%A8%8B.png)

#### Note2(5)

1. 进程之间不能共享内存
2. 主进程需要等待子进程结束，主进程负责**创建**和**回收**子进程
3. 子进程执行结束，若父进程没有回收资源，即**僵尸**进程。
4. 主进程结束逻辑：主进程**代码结束**、所有子进程结束、回收子进程资源、主**进程结束**
5. **进程里面无法进行input**
   - 所有print都是向文件里**写**数据
   - 只有主进程支持input操作，子进程不支持，会报错**EOFError: EOF when reading a line**

#### 2.3 join方法

- 把一个进程的结束事件封装成一个join方法
- 执行join方法效果，**阻塞**，直到子进程结束，就结束
- 所有的进程执行的先后是由**OS控制的**

```python
# 在多个子进程中使用join方法
from multiprocessing import Process
def send_mail(i):
    print('邮件已发送', i)
if __name__ == '__main__':
    li = []
    for i in range(10):
        p = Process(target=send_mail, args=(i,))  # args必须是元组，给子进程中的函数传参数
        p.start()
    li.append(p)
    for p in li: p.join()													# 阻塞，知道所有子进程执行完毕
    print('100封邮件已发送')
# 主线程等待p终止（强调：是主线程处于等的状态，而p是处于运行的状态）。timeout是可选的超时时间，需要强调的是，p.join只能join住start开启的进程，而不能join住run开启的进程 
```



## 9.3 锁&进程间通信

### 1. Process类

#### 1.1 守护进程

- 生产者消费者模型
- 和守护线程做对比

 **p.daemon**：默认值为False，如果设为True，代表p为后台运行的守护进程，当p的父进程终止时，p也随之终止，并且设定为True后，**p不能创建自己的新进程**，必须在p.start()之前设置

```python
import time
from multiprocessing import Process

def son1():
  	while True:
        print('is alive')
        time.sleep(0.5)
    
def son2():
  	for i in range(5):
      print('in son2')
      time.sleep(1)
              
if __name__ == '__main__':
    p = Process(target=son1)
    p.daemon = True                               # 把p子进程设置成了守护进程	 
    p.start()
    p2 = Process(target=son2)
    p2.start()
    time.sleep(2)
# 守护进程是随着主进程‘代码’结束而结束
# 所有子进程都必须在主进程结束之前结束
# 守护进程内无法再开启子进程,否则抛出异常：AssertionError: daemonic processes are not allowed to have children
```

#### 1.2 Process的方法

- p.**terminate**(), p.i**s_alive**()，current_process(current_process.ident/ **name**/ pid)
- **异步非阻塞**
  - 使用terminate方法后，再查看进程是否还存活时，会发现进程还存活，并没有等待OS关闭进程，说明terminate方法请求后，程序会继续执行

```python
import time
from multiprocessing import Process

def son1():
  	while True:
        print('is alive')
        time.sleep(0.5)
              
if __name__ == '__main__':
    p = Process(target=son1)
    p.start()                   # 开启了一个进程
  	print(p.is_alive)           # 判断子进程时候存活, Ture和False
    time.sleep(1)
    p.terminate()               # “异步非阻塞”，强制结束一个子进程
    print(p.is_alive)           # True，os还没来得及关闭进程
   	time.sleep(0.01)
    print(p.is_alive)           # False，OS已经响应了关闭进程的需求，再去检测的时候，结果是进程已经结束
```

#### 1.3 面向对象开启进程

- 当创建子进程需要传参时，需要使用**super()._\_init__()**

```python 
import os
import time
from multiprocessing import Process

class MyProcess(Process):
    def __init__(self, x, y):                 # 子进程如果不需要参数，可以省略
        self.x = x
        self.y = y
        super().__init__()        
        
    def run(self):
        while True:
            print(self.x, self.y, os.getpid())
            print('in myprocess')

if __name__ == '__main__':
    mp = MyProcess(1, 2)
    mp.daemon = True
    mp.start()                                # 开启一个子进程，会调用run()方法
    time.sleep(1)
    mp.terminate()						              	# 结束进程，异步非阻塞						
    print(mp.is_alive())				              # True
    time.sleep(0.01)				
    print(mp.is_alive())				              # False
```

- p.join() : 同步阻塞
- p.terminate() 和 p.start()：异步非阻
- p.is_alive()：获取当前进程状态
- **daemon = True**：设置为守护进程，守护进程在主进程代码执行结束而结束

### 2. 锁

1. 在并发的场景下，设计某部分的内容
   - 需要修改一些所有进程共享数据资源
   - 需要加锁来维护数据的安全
2. 在数据安全的基础上，才考虑效率问题
   - with lock：内部**有异常处理**
   - 在主进程中进行实例化
   - 把锁传递给子进程
3. 在子进程中对需要加锁的代码进行with lock
   - with lock：相当于lock.acquire()和lock.release()
4. 需要加锁的场景
   - 操作共享的数据资源
   - 对资源进行修改操作
   - 加锁之后能够保证数据的安全性，但会降低程序执行效率

```python 
# 数据操作时，不能同时进行修改
import json
from multiprocessing import Process, Lock             # 导入Lock

def search_ticket(user):
    with open('tickets.txt') as f:
        dic = json.load(f)
        print('%s查询结果：%s张余票' %(user, dic['count']))

def buy_ticket(user, lock):
    # with lock:
    lock.acquire()
    # time.sleep(0.01)
    with open('tickets.txt') as f:
        dic = json.load(f)
    if dic["count"] > 0:
        print('%s已买到票' % user)
        dic["count"] -= 1
    else:
        print('%s没买到票' % user)
    with open('tickets.txt', 'w') as f:
        json.dump(dic, f)
    lock.release()


if __name__ == '__main__':
    lock = Lock()                                      # 实例化一个对象
    for i in range(10):
      	search_ticket('user%s '%(i+1),)
        p = Process(target=buy_ticket, args=('user%s '%(i+1), lock))
        p.start()
```

### 3. 进程间的通信

#### 3.1 进程间数据隔离

```python
from multiprocessing import Process
n = 100
def func():
	global n
  n -= 1
 
li = []
for i in range(10):
  p = Process(target=func)
  p.start()
  li.append(p)
  
 for p in li:p.join()
 print(n)
```

#### 3.2 进程间通信

- 文件或网络只有这两种
- 进程间通信(**IPC**， inter-process communication):**Queue和Pipe**
- **Queue(3)**：先进先出，文件家族的**socket**，写文件基于**pickle**，基于**Lock**
  - 数据安全，**Pipe**管道：天生数据不安全（socket通信）
  - Queue = **Pipe**(socket + picket)**+Lock**
- **第三方提供(5)**：redis，memcache，kafka，rabbitmq（消息中间件(消息转发)）
  - 并发需求
  - 高可用
  - 实现集群的概念
  - 断电保存数据
  - 解耦

```python
from multiprocessing import Process,Queue

def func(exp,q):
    res = eval(exp)
    q.put(res)

if __name__ == '__main__':
    q = Queue()
    p = Process(target=func, args=('1+2+3',q))
    p.start()
    print(q.get())
```

```python
from multiprocessing import Pipe
pip = Pipe()
pip.send()
pip.recv()
```

```python
# Process中的队列
import queue
from multiprocessing import Queue
q = Queue(3)													# 可设置队列长度
q.put(1)
q.put(2)															# 对列为满时，继续放数据会发生阻塞
q.put(3)
print('----------')
try:
	q.put_nowait(4)                     # 对列为满时，继续放数据会报错和丢失
except queue.Full:pass
print('----------')

q.get()
q.get()
q.get()                                # 对列为空时，会发生阻塞
try:
	q.get_nowait()											 # 对列为空时，会报错，阻塞会取消
except queue.Empty:pass
```

```python
q.empty()                              # 有缺陷
q.qsize()
q.full()
```



## 9.4 cp模型&线程

### 1. 生产者消费者模型

#### 1.1 程序的解耦

- 把写在一起的功能分开成多个小的功能处理
  - 修改和复用，增加可读性
  - 计算速度有差异，执行效率最大化，节省进程
- **生产者**：生产数据
- **消费者**：处理数据

![生产者消费者模型](/Users/henry/Documents/%E6%88%AA%E5%9B%BE/Py%E6%88%AA%E5%9B%BE/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B.jpg)

#### 1.2 生产者和消费者

1. 一个进程就是一个生产者/消费者
2. 生产者和消费者之间的容器就是队列(**队列有大小，控制内存消耗**)

```python
# 生产者消费者模型示例
import time
import random
from multiprocessing import Process, Queue

def producer(q, name, food):
    for i in range(10):
        time.sleep(random.random())
        fd = '%s%s' % (food, i)
        q.put(fd)
        print('%s生产了一个%s' % (name, food))

def consumer(q, name):
    while True:
        food = q.get()
        if not food:
            q.put(None)
            break
        time.sleep(random.randint(1, 3))
        print('%s吃了%s' % (name, food))

def cp(num1, num2):
    q = Queue(10)
    p_l = []
    for i in range(num1):
        p = Process(target=producer, args=(q, 'henry', 'food'))
        p.start()
        p_l.append(p)
    for i in range(num2):
        c = Process(target=consumer, args=(q, 'echo%s' % (i+1,)))
        c.start()
    for i in p_l:
        i.join()
    q.put(None)

if __name__ == '__main__':
    cp(1, 4)
```

```python
# 生产者消费者模型示例之爬虫
import re
import requests
from multiprocessing import Process, Queue

def producer(q, url):
    response = requests.get(url)
    q.put(response.text)

def consumer(q):
    while True:
        s = q.get()
        if not s:
            q.put(None)
            break
        com = re.compile(
            '<div class="item">.*?<div class="pic">.*?<em .*?>(?P<id>\d+).*?<span class="title">(?P<title>.*?)</span>'
            '.*?<span class="rating_num" .*?>(?P<rating_num>.*?)</span>.*?<span>(?P<comment_num>.*?)评价</span>', re.S)
        ret = com.finditer(s)
        for i in ret:
            print({
                "id": i.group("id"),
                "title": i.group("title"),
                "rating_num": i.group("rating_num"),
                "comment_num": i.group("comment_num"),
            })

if __name__ == '__main__':
    count = 0
    q = Queue()
    p_l = []
    for i in range(10):
        count += 25
        p = Process(target=producer, args=(q, 'https://movie.douban.com/top250?start=%s&filter=' % count))
        p.start()
        p_l.append(p)
    for i in range(5):
        c = Process(target=consumer, args=(q,))
        c.start()
    for i in p_l:
        i.join()
    q.put(None)
```

#### 1.3 joinablequeue

1. **q.join()**：阻塞，直到队列中所有内容被取走且**q.task_done**
   - 生产者将使用此方法进行阻塞，直到队列中所有项目均被处理。阻塞将持续到为队列中的每个项目均调用
2. 先设置消费者为守护进程
   - **c.daemon = True**
3. 阻塞生产者
   - 其中的队列阻塞结束后，才会结束
4. 在生产者中使用阻塞队列
   - 阻塞一结束，所有数据都已经消费完
5. 队列阻塞结束代表消费者，把所有生产数据消费完（**jq.taks_done()操作**）
   - 使用者使用此方法发出信号，表示q.get()返回的项目已经被处理。如果调用此方法的次数大于从队列中删除的项目数量，将引发**ValueError**异常。

![joinable_queue](/Users/henry/Documents/%E6%88%AA%E5%9B%BE/Py%E6%88%AA%E5%9B%BE/joinable_queue.png)

![joinable逻辑](/Users/henry/Documents/%E6%88%AA%E5%9B%BE/Py%E6%88%AA%E5%9B%BE/joinable%E9%80%BB%E8%BE%91.png)

```python
# joinable实现生产者、消费者模型
import time
import random
from multiprocessing import Process,JoinableQueue

def producer(q, name, food):
  for i in range(10):
    time.sleep(random.random())
    fd = '%s%s'%(food,i)
    print('%s生产了一个%s'%(name, food))
  q.join()

def consumer(q, name, food):
  while True:
    food = q.get()
    if not food:
      q.put(None)
      break
    time.sleep(random.randint(1, 3))
    print('%s吃了%s'%(name, food))
    q.task_done()
 
if __name__ = '__main__':
  jq = JoinableQueue()
  p = Processor(target=producer, args=(jq, 'henry', 'food'))
  p.start()
```

### 2. 进程间数据共享

1. 与数据共享相关：**Manager模块**(Manager().list(), Manager().Queue)
2. multiprocessing 中有一个manager类
3. 封装了所有和进程相关的**数据共享**、**数据传递**
4. 但是对于dict、list这类进行数据操作时，会产生数据不安全
5. m = Manager()也可以使用**with Manager() as m**:

```python
# 进程间数据是独立的，可以借助于队列或管道实现通信，二者都是基于消息传递的
# 虽然进程间数据独立，但可以通过Manager实现数据共享，事实上Manager的功能远不止于此
A manager object returned by Manager() controls a server process which holds Python objects and allows other processes to manipulate them using proxies.

A manager returned by Manager() will support types list, dict, Namespace, Lock, RLock, Semaphore, BoundedSemaphore, Condition, Event, Barrier, Queue, Value and Array.
```

```python
from mutliprocessing import Manager,Lock
def func(dic, lock):
  with lock:
		dic['count'] -= 1

if __name__ = '__main__':
  # m = Manager()
  lock = Lock()
  with Manager() as m:
    l = Lock()
    dic = m.dict({'count': 100})    							# 共享的dict,list li = m.list([1,2,3])
    p_l = []
    for i in range(100):
      p = Process(target=func, args=(dic,lock))
      p.start()
      p_l.append(p)
    for p in p_l:p.join()
    print(dic)
```

### 3. 线程

1. **调度和切换**：线程**上下文切换**比进程上下文切换要快得多。
4. **四核八线程**
   - 每个核心被虚拟成两个核心，可以同时执行8个线程。
   - 如果是计算复杂数据，会转换到四核
   - https：加证书，需要购买
5. 在多线程的操作系统中，通常是在一个进程中包括多个线程，**每个线程**都是作为**利用CPU的基本单位**，是花费最小开销的实体。
6. **线程具有以下属性(4)**
  
   1. 线程中的实体基本上不拥有系统资源，只是**有一点必不可少的、能保证独立运行的资源**。
   
      - 独立调度和分派的基本单位。
   
      - 在多线程OS中，线程是能独立运行的基本单位，因而也是独立调度和分派的基本单位。由于线程很“轻”，故线程的切换非常迅速且开销小（在同一进程中的）。
   
   2. 线程的实体包括**程序**、**数据**和**TCB**。**线程是动态概念**，它的动态特性由线程控制块**TCB**（Thread Control Block）描述。
      - 线程状态。
      - 当线程不运行时，被保存的现场资源。
      - 一组执行堆栈。
      - 存放每个线程的**局部变量主存区**。
      - 访问同一个进程中的主存和其它资源。
      - 用于指示被执行指令序列的**程序计数器**、**保留局部变量**、**少数状态参数**和**返回地址**等的一组寄存器和堆栈。
   
   3. **共享进程资源**
   
      - 线程在同一进程中的各个线程，都可以共享该进程所拥有的资源，
      - 这首先表现在：**所有线程**都具有**相同的进程id**，这意味着，线程可以访问该进程的每一个内存资源；
      - 此外，还可以访问进程所拥有的**已打开文件**、**定时器**、**信号量机构**等。由于同一个进程内的线程共享内存和文件，所以线程之间互相通信不必调用内核。
   
   4. **可并发执行**
   
      - 在一个进程中的多个线程之间，可以并发执行，甚至允许在一个进程中所有线程都能并发执行；
      - 同样，不同进程中的线程也能并发执行，充分利用和发挥了处理机与外围设备并行工作的能力。

#### 3.1垃圾回收机制

1. **cpython**解释器不能实现多线程利用多核
2. 垃圾回收机制(gc)：**引用计数** + **分代回收**
   - 专门有一条线程完成垃圾回收机制，对每一个在程序中的变量**统计引用计数**

#### 3.2 GIL锁

**GIL**(global interpreter lock)：**全局解释器锁**

1. 保证了整个python程序中，只能有一个线程被CPU执行
   - 导致了py程序不能并行
   - 使用多线程并不影响高IO型操作，只会对高计算型程序有效率的影响
   - 遇到高计算：多进程+多线程，分布式
2. 原因：**cpython**解释器中特殊的垃圾回收机制
3. cpython、pypy，jpython(先翻译为java字节码，在java上执行)、iron python

#### 3.3 遇到IO操作的时候

1. 5-6亿条cpu指令
2. 5-6cpu指令 == 一句python代码
3. 几千万条python代码

#### 3.4 web框架几乎都是多线程

- 利用IO操作，类似多道系统

### 4. python操作线程(重点)

#### 4.1 开启线程

- 使用**Threading**类

```python
# multiprocessing 是完全仿照Threading类的
import os
import time
from threading imoprt Thread
def func():
  print('start son thread')
  time.sleep(1)
  print('end son thread', os.getpid())
  
Thread(target=func).start()                 # 开启一个线程的速度非常快
print('start')
time.sleep(0.5)
print('end', os.getpid())
```

```python
# 开启多个线程
import os
import time
from threading imoprt Thread
def func():
  print('start son thread')
  time.sleep(1)
  print('end son thread', os.getpid())
 
for i in range(10):
  Thread(target=func).start()                 # 开启一个线程的速度非常快
  																					  # 线程的调度由OS决定
```

#### 4.2 join方法

1. join阻塞知道子线程执行结束

- **主线程**如果结束了，**主进程**也就结束了
- **主线程**需要等待**所有子线程结束**才能结束

```python
import os
import time
from threading imoprt Thread
def func():
    print('start son thread')
    time.sleep(1)
    print('end son thread', os.getpid())

t_l = []
for i in range(10):
    t = Thread(target=func)
    t.start()
    t_l.append(t)
for i in t_l:i.join()
print('子线程执行结束')
```

#### 4.3 面向对象启动线程

- self.ident / current_thread：查看线程id
- enumerate / active_count：查看线程存活情况

```python
from threading import Thread

class MyThread(Thread):
    def __init__(self, i):
        self.i = i
        super().__init__()                   # 注意变量名避免与父类init中的变量重名  

    def run(self):
        print(self.i, self.ident)            # 通过self.ident，查看子线程的id

t_l = []
for i in range(100):
    t = MyThread(i)
    t_l.append(t)
    t.start()                                 # start 方法封装了开启线程的方法和run方法

for t in t_l: t.join()
print('主进程结束')       
```

#### 4.4 线程中的其他方法(3大类)

```python
from threading import current_thread, enumerate, active_count

def func():
    print('start son thread', i , current_thread())
    time.sleep(1)
    print('end son thread', os.getpid())

t = Thread(target=func)
t.start()
print(t.ident)
print(current_thread().ident)                 # current_ident()在哪个线程，就得到这个线程id
print(enumerate())					                  # 统计当前进程中多少线程活着，包含主线程
print(active_count())				                  # 统计当前进程中多少线程活着，个数，包含主线程
                                              # 线程中不能从主线程结束一个子线程
  
  
current_thread().name     									  # 当前线程名称
current_thread().ident												# 当前线程id
current_thread().isalive()										# 当前线程是否存活
current_thread().isdaemon()										# 当前线程是否是守护线程
```

#### 4.5 效率差

```python
import time
from threading import Thread
from multiprocessing import Process
def func(a, b):
  	c = a + b
 
if __name__ == '__main__':
    p_l = []
    start = time.time()
    for i in range(100):
        p = Process(target=func, args=(i, i*2))
        p.start()
        p_l.append(p)
     for i in P_l:i.join()
     print(time.time() - start)
    
     t_l = []
     start = time.time()
     for i in range(100):
         t = Thread(target=func, args=(i, i*2))
         t.start()
         t_l.append(t)
     for i in t_l:i.join()
     print(time.time() - start)
```

#### 4.6 数据共享

```python
# 不要在子线程里随便修改全局变量
from threading import Thread
n = 100
def son():
  global n
  n -= 1

t_l = []
for i in range(100):
		t = Thread(target=son)
    t_l.append(t)
    t.start()
    
for t in t_l:t.join()
print(n)
```

#### 4.7 守护线程

- 守护线程会一直等到所有非守护线程结束之后才结束
- 除了**守护主线程**的代码之外，也会**守护子线程**
- 只要有非守护线程存在，主进程就不会结束

```python
import time
from threading imoprt Thread
def son():
  	while True:
      		time.sleep(0.5)

def son2():
  	for i in range(5):
      
t = Thread(target=son)
t.daemon = True
t.start()
time.sleep(3)
```

#### Note3(2)

1. 对**主进程**来说，运行完毕指的是主进程**代码运行完毕**
   - 主进程在其代码结束后就已经运行完毕了（守护进程在此时就被回收），然后主进程会一直等非守护进程都运行完毕后回收子进程资源（否则会产生僵尸进程），才会结束。
2. 对**主线程**来说，运行完毕指的是主线程所在的进程内所有**非守护线程执行完毕**
   - 主线程在其他非守护线程运行完毕后才算运行完毕（守护线程在此时就被回收），因为主线程的结束意味着进程的结束，进程整体的资源都将被回收，而进程必须保证非守护线程都运行完毕后才能结束。

## 9.5 锁&池

### 1. 互斥锁

#### 1.1 互斥锁

```python
# 线程数据同样不安全
import dis
a = 0
def func():
  	global a
  	a += 1
dis.dis(func)                                   # 返回cpu指令
```

- 即便是线程，有GIL锁， 也会出现**数据不安全**的问题
- **STORE_GLOBAL**：一旦有这种方法，就会有数据安全问题
- **操作是全局变量**
- **操作以下方法**
  - += , -= , *=, /=（在操作线程全局变量时，注意）
  - li[0] += 1, dic['key'] -= 1
  - 对同一文件进行写操作

```python
# 使用互斥锁解决线程全局变量数据不安全问题
from threading import Thread, Lock
a = 0
def add_f(lock):
    global a
    with lock:
        for i in range(2000000):
            a += 1
def sub_f(lock):
    global a
    with lock:
        for i in range(2000000):
            a -= 1
lock = Lock()
t1 = Thread(target=add_f, args=(lock,))
t1.start()
t2 = Thread(target=sub_f, args=(lock,))
t2.start()
t1.join()
t2.join()
print(a)
```

**互斥锁**：是锁中的一种，在同一线程中，不能连续lock.acquire()多次

```python
from threading import Lock
lock = Lock()
lock.acquire()
print('-------------')
lock.acquite()
print('-------------')
```

#### 1.2 单例模式

```python 
import time
import random
from threading import Thread

class Singleton:
    from threading import Lock                          # 复用性考虑
    __instance = None
    lock = Lock()
    
    def __new__(cls, *args, **kwargs):
        with cls.lock:
            if not cls.__instance:
                time.sleep(random.random())             # 切换GIL锁
                cls.__instance = super().__new__(cls)
        return cls.__instance

def fun():
    print(Singleton())

li = []
for i in range(10):
    t = Thread(target=fun)
    li.append(t)
    t.start()
for t in li: t.join()
```

### 2. 死锁

#### 2.1 原因(2)

- 有**多把锁**(一把以上)
- 多把锁**交替使用**

```python
from threading import Thread,Lock
noodle_lock = Lock()
fork_lock = Lock()
def eat1(name,noddle_lock, fork_lock):
    noddle_lock.acquire()
    print('%s抢到面了'%name)
    fork_lock.acquire()
    print('%s抢到叉子了'%name)
    print('%s吃了一口面'%name)
    noddle_lock.release()
    print('%s放下面了'%name)
    fork_lock.release()
    print('%s放下叉子了'%name)
 
def eat2(name,noddle_lock, fork_lock):
    fork_lock.acquire()
    print('%s抢到叉子了'%name)
    noddle_lock.acquire()
    print('%s抢到面了'%name)
    print('%s吃了一口面'%name)
    noddle_lock.release()
    print('%s放下面了'%name)
    fork_lock.release()
    print('%s放下叉子了'%name)
```

#### 2.2 解决方案

1. **递归锁**
   - 递归锁在同一线程中，可以连续**acquire多次**不会阻塞
   - **本质**：一把锁
   - acquire和release次数要一致
   - **优点**：在同一线程中多次acquire也不会发生阻塞
   - **缺点**：**占用**了更多的**资源**
2. **多把递归锁**也会产生**死锁**现象
   - 使用递归锁，永远不要使用多把
   - 互斥锁效率更高，递归锁效率较低

```python
import time
from threading import RLock, Thread
noodle_lock = fork_lock = RLock()                      # 将多把互斥锁变成了一把递归锁

def eat1(name, noodle_lock, fork_lock):
    noodle_lock.acquire()
    print('%s抢到面了' % name)
    fork_lock.acquire()
    print('%s抢到叉子了' % name)
    print('%s吃了一口面' % name)
    time.sleep(0.1)
    fork_lock.release()
    print('%s放下叉子了' % name)
    noodle_lock.release()
    print('%s放下面了' % name)

def eat2(name, noodle_lock, fork_lock):
    fork_lock.acquire()
    print('%s抢到叉子了' % name)
    noodle_lock.acquire()
    print('%s抢到面了' % name)
    print('%s吃了一口面' % name)
    time.sleep(0.1)
    noodle_lock.release()
    print('%s放下面了' % name)
    fork_lock.release()
    print('%s放下叉子了' % name)

lst = ['henry', 'echo', 'dean', 'daniel']
Thread(target=eat1, args=(lst[0], noodle_lock, fork_lock)).start()
Thread(target=eat2, args=(lst[1], noodle_lock, fork_lock)).start()
Thread(target=eat1, args=(lst[2], noodle_lock, fork_lock)).start()
Thread(target=eat2, args=(lst[3], noodle_lock, fork_lock)).start()
```

3. **代码优化**

```python
# 使用互斥锁解决问题
import time
from threading import Lock, Thread

lock = Lock()
def eat1(name, lock):
    lock.acquire()
    print('%s抢到面了' % name)
    print('%s抢到叉子了' % name)
    print('%s吃了一口面' % name)
    time.sleep(0.1)
    print('%s放下叉子了' % name)
    print('%s放下面了' % name)
    lock.release()

def eat2(name, lock):
    lock.acquire()
    print('%s抢到叉子了' % name)
    print('%s抢到面了' % name)
    print('%s吃了一口面' % name)
    time.sleep(0.1)
    print('%s放下面了' % name)
    print('%s放下叉子了' % name)
    lock.release()

lst = ['henry', 'echo', 'dean', 'daniel]
Thread(target=eat1, args=(lst[0], lock)).start()
Thread(target=eat2, args=(lst[1], lock)).start()
Thread(target=eat1, args=(lst[2], lock)).start()
Thread(target=eat2, args=(lst[3], lock)).start()
```

### 3. 队列

- 线程之间的通信，线程安全

```python
import queue
# 先进先出队列：服务
from queue import Queue           
q = Queue(5)
q.put(1)
q.get()
# 后进先出队列：算法
from queue import LifoQueue
lfq = LifiQueue(4)
lfq.put(1)
lfq.put(2)
lfq.get()
lfq.get()
# 优先级队列：自动排序、vip用户、告警级别
from queue import PriorityQueue
pq = PriorityQueue()
pq.put((10, 'henry'))
pq.put((6, 'echo'))
pq.put((10, 'dean'))
pq.get()
pq.get()
pq.get()
```

- FIFO：所有的请求放在对列里，先来先服务思想
- LIFO：一般用于算法

### 4. 池

- 进程，线程开启关闭切换需要时间
- 进程池一般和cpu核心说有关，个数一般为cpu核心数或加一
- 节省了进程创建和销毁的时间

#### 4.1 池

- 预先开启固定个数的进程数，当任务来临时，直接提交给开好的进程，让这个进行执行，从而减轻了os调度的负担

#### 4.2 concurrent

- **concurrent.futures模块(3)**
- 3.4版本之后出现
- **进程池**

```python
# 进程池。 p.submit， p.shutdown
import os,time, random
from concurrent.futrures import ProcessPoolExecutor

def func(i):
    print('start', os.getpid())
    time.sleep(random.randint(1,3))
    print('end', os.getpid())
    return '%s * %s' %(i, os.getpid())

if __name__ == '__main__':
    p = ProcessPoolExecutor(5)                    # cpu核心数或多一
    ret_l = []
    for i in range(10):
        ret = p.submit(func, i)			              # 提交进程,参数直接放在其后
        ret_l.append(ret)						              # ret为future对象，ret.result()取值
    # 关闭池，不能提交任务，阻塞，直到已经提交的任务执行完毕
    p.shutdown()
    for ret in ret_l:                             # 会阻塞，相当于q.get()
      	print('------>',ret.result())             # result，同步阻塞
    print('main', os.getpid())
```

- 一个进程池中的任务个数，限制了我们程序的并发个数
- **线程池**

```python
# 线程池。p.submit(), p.shutdown(), ret.result()
from concurrent.futures import ThreadPoolExecutor

def func(i):
    print('start', os.getpid())
    time.sleep(random.randint(1,3))
    print('end', os.getpid())
    return '%s * %s' %(i, os.getpid())

tp = ThreadPoolExecutor(20)                      # 线程个数一般为cpu核心数4-5倍
ret_l = []
for i in range(100):
		ret = tp.submit(func, 1)
    ret_l.append(ret)
for ret in ret_l:
  	print('------->', ret.result())
p.shutdown()
print('main')
```

- tp.map(func, **可迭代对象**)：参数只能传输一个

```python
# 线程池。p.submit(), p.shutdown(), ret.result()
from concurrent.futures import ThreadPoolExecutor

def func(i):
    print('start', os.getpid())
    time.sleep(random.randint(1,3))
    print('end', os.getpid())
    return '%s * %s' %(i, os.getpid())

tp = ThreadPoolExecutor(20)                     # 线程个数一般为cpu核心数4-5倍
ret = tp.map(func, range(20))				          	# tp.map()方法
for i in ret:print(i)														# ret 为生成器对象
```

#### 4.3 回调函数

- ret.add_done_callback：回调函数
- 先来先响应，会提高整体的处理速度
- 会监听obj值：直到obj.result()有值为止

```python
from concurrent.futures import ThreadPoolExecutor
def get_page(url):
  	content = requests.get(url)
    return {'url':url, 'content':content.text}
def parserpage(dic):
  	print(dic.result()['url'])

for url in url_l:
  	ret = get_page(url)
    ret.add_done_callback(parserpage)          # 先执行完，先调用parserpage函数
    																	       	 # ret=add_done_callback(函数名)
```

```python
from concurrent.futures import ProcessPoolExecutor
import requests
import os

def get_page(url):
    print('<进程%s> get %s' % (os.getpid(), url))
    respone = requests.get(url)
    if respone.status_code == 200:
        return {'url': url, 'text': respone.text}

def parse_page(res):
    res = res.result()
    print('<进程%s> parse %s' % (os.getpid(), res['url']))
    parse_res = 'url:<%s> size:[%s]\n' % (res['url'], len(res['text']))
    with open('db.txt', 'a') as f:
        f.write(parse_res)

if __name__ == '__main__':
    urls = ['https://www.baidu.com', 'https://www.openstack.org', 'http://www.sina.com.cn/', 'https://www.tencent.com']
    p = ProcessPoolExecutor(3)
    for url in urls:
        p.submit(get_page, url).add_done_callback(parse_page)  # parse_page拿到的是一个future对象obj，需要用obj.result()拿到结果

```



## 9.6 协程

### 1. 协程概念

1. **协程**：是单线程下的并发，又称**微线程**，**纤程**。英文名**Coroutine**。一句话说明什么是协程：**协程是一种用户态的轻量级线程，即协程是由用户程序自己控制调度的**，多个任务在一条线程上来回切换。

2. **协程**：用户级别，自己写的py代码；控制切换是OS不可见的

   1. 一个线程中的阻塞都被其他的各种任务占满了
   2. 让OS认为这个线程很忙，尽量减少这个线程进入阻塞状态
   3. 提高了**单线程对cpu的利用率**
      - 多个任务在同一个线程中执行，也达到了一个并发的效果
   4. 规避了每个任务的io操作，减少了线程的个数，减轻了OS负担

3. 在**Cpython**解释器：**协程和线程**都不能利用**多核**

   1. 由于多线程本身不能利用多核
   2. 即便开启了多线程也只能**轮流在一个cpu上执行**
   3. 协程如果把所有**io操作都规避掉**，只剩下需要使用cpu的操作

4. 线程和协程**对比**

   1. **线程**
      - 切换需要OS，开销大，os不可控，给os的压力大
      - os对io操作的感知更加敏感
   2. **协程**
   - 切换需要py代码，开销小，用户操作可控，完全不会增加os压力
      - 用户级别对io操作感知较低
        - 协程切换开销几乎和函数调用一致
   
5. **协程特点**

   1. 必须在只有一个单线程里实现并发
   2. **修改共享数据**不需加锁
   3. 用户程序里自己保存多个控制流的上下文栈
   4. 附加：一个协程遇到IO操作自动切换到其它协程（如何实现检测IO，yield、greenlet都无法实现，就用到了gevent模块（**select机制**））

6. 对比OS控制线程的切换，用户在**单线程内**控制协程的切换

   **优点如下**：

   1. 协程的切换开销更小，属于程序级别的切换，操作系统完全感知不到，因而更加轻量级
   2. 单线程内就可以实现并发的效果，最大限度地利用cpu

   **缺点如下**：

   1. 协程的**本质**是单线程下，无法利用多核，可以是一个程序开启多个进程，每个进程内开启多个线程，每个线程内开启协程
   2. **协程指的是单个线程**，因而一旦协程出现阻塞，将会**阻塞整个线程**

### 2. greenlet&gevent

#### 2.1 原生python完成

- asynicio：**使用yield**
- 能够在一个线程下的多个任务之间来回切换，那么每一个任务都是协程

```python
# 切换，协程任务
def func1():
  	1 + 2
    2 + 3
    yield 1 
    sleep(1)
def func2():
  	g = func1()
    next(g)
    next(g)
```

#### 2.2 C语言完成的py模块

- greenlet
- gevent

1. greenlet模块

```python
# 完成切换
import time
from greenlet import greenlet

def func1():
    print(123)
    time.sleep(0.5)
    g2.switch()
    print(123)
  
def func2():
  	print(456)
    time.sleep(0.5)
    print(456)
    g1.switch()
   
g1 = greenlet(func1)
g2 = greenlet(func2)
g1.switch()
```

2. **协程切换原理**
   - **事件循环**：第三者一直在循环所有任务调度所有任务

```python
def sleep(num):
  	t = num + time.time()
    yield t
    print('sleep 结束')
    
g1 = sleep(1)
g2 = sleep(2)
t1 = next(g1) 
t2 = next(g2)
lst = [t1, t2]
min_t = min(lst)
time.sleep(min_t - time.time())
g1.next()

min_t = min(lst)
time.sleep(min_t - time.time())
g2.next()
```

3. gevent模块
   - 基于greenlet
   - gevent.sleep/ gevent.**spawn(func)** / gevent.**joinall(func_li)** / g.**join**()

```python
# gevent不认识time.sleep
import time
import gevent
from gevent import monkey
monkey.patch_all()                        # 可能的阻塞方法

def func1():
    print(123)
    gevent.sleep(1)
    print(123)
  
def func2():
  	print(456)
    gevnet.sleep(1)
    print(456)
  
g1 = gevent.spawn(fun1)
g2 = gevent.spawn(fun2)
# gevent.sleep(2)											  	# 遇到阻塞才会切换
# g1.join()                               # 阻塞直到g1任务完成为止
# g2.join() 
gevent.joinall([g1, g2])

g_l = []
for i in range(10):
  	g = gevent.spawn(func1)
    g_l.append(g)
gevent.joinall(g_l)
```

```python
# 示例大变身
import gevent
from gevent import monkey,time
monkey.patch_all()                          # 对io操作进行重现实现

def func1():
    print(123)
    time.sleep(3)
    print(456)

def func2():
    print('---------')
    time.sleep(1)
    print('=========')

g1 = gevent.spawn(func1)
g2 = gevent.spawn(func2)
gevent.joinall([g1, g2])									  # 阻塞列表中的所有协程
print('main')
```

- 获取返回值

```python
print(g1.value )                            # value是属性，如果没有执行则为None
```

- 协程实现socket

```python
import socket
import gevent
from gevent import monkey
monkey.patch_all()

sk = socket.socket()
sk.bind(('127.0.0.1', 9000))
sk.listen()

def chat(con):
    while True:
        msg = con.recv(1024).decode('utf-8')
        con.send(msg.upper().encode('utf-8'))

while True:
    con, _ = sk.accept()
    gevent.spawn(chat, con)
```

### 3. asynico

- python原生底层的协程模块
  - 爬虫、webserver框架
  - 提高网编的效率和并发效果

#### 3.1 启动一个任务

```python
import asyncio                          # 只识别自身的方法

# 起一个任务
async def demo():                       # 必须要async修饰，协程方法
    print('start')
		await asyncio.sleep(1)              # 阻塞，await关键字 + 协程函数
    print('end')
    
loop = asyncio.get_event_loop()         # 创建一个事件循环
loop.run_until_complete(demo())         # 阻塞，直到协程执行完毕
																				# 把demo任务丢到事件循环中执行
```

#### 3.2 启动多个任务

1. **没有返回值**
   - 创建一个事件循环
   - 等待
   - 阻塞

```python
import asyncio                          # 只识别自身的方法

async def demo():                       # 必须要async修饰，协程方法
    print('start')
		await asyncio.sleep(1)              # 阻塞，await关键字 + 协程函数
    print('end')

loop = asyncio.get_event_loop()         # 创建一个事件循环
wait_obj = asyncio.wait([demo(), demo(), demo()])
loop.run_until_complete(wait_obj)       # 没有返回值
```

2. **有返回值**
   - 创建一个事件循环
   - loop.creat_task(func(arg1, arg2 …))
   - asyncio.wait([task1, taks2 …])：得到一个任务列表对象
   - loop.run_until)_complete(wait_obj)
   - task列表中即为返回值(**task对象**)

```python
# 同步取返回值
import asyncio
async def demo():
    print('start')
		await asyncio.sleep(1)
    print('end')
    return 123
    
loop = asyncio.get_event_loop()        # 创建一个事件循环
t1 = loop.create_task(demo())
t2 = loop.create_task(demo())
tasks = [t1, t2]                       # 任务列表
wait_obj = asyncio.wait([t1, t2])
loop.run_until_complete(wait_obj)
for task in tasks:
 	 	print(t.result())                   # 获取返回值
```

3. **异步取返回值**
   - task = asyncio.ensure_future(func(arg1, arg2...))
   - asyncio.as_compeleted(task_l)：可迭代对象
   - i await i

```python
import asyncio
async def demo(i):
    print('start')
		await asyncio.sleep(10-i)
    print('end')
    return i, 123

async def main():
  	task_l = []
    for i in range(10):
      	task = asyncio.ensure_future(demo(i))
        task_l.append(task)
    for ret in asyncio.as_compeleted(task_l):
      	res = await ret
        print(res)
        
loop = asyncio.get_event_loop() 
loop.run_until_compeleted(main())
```

#### Note3(2)

1. **await** 阻塞事件，协程函数从这里切换出去，还能保证切回来
   - 必须写在**async**里面，async函数是个协程函数(调用时并不执行)
2. **loop**是事件循环，所有的协程执行、调度、都离不开**loop**

### 4. 协程的上下文切换

1. 在一个基于协程的应用程序中，可能会产生数以千计的协程，所有这些协程，会有一个的**调度器来统一调度**。
2. 另外我们知道，高性能的程序首要注意的就是**避免程序阻塞**。
3. 那么，在以**协程为最小运行单位**的程序中，同样也需要确保这一点，即每一个协程都不能发生阻塞。
4. 因为**只要某一个协程发生了阻塞**，那么整个调度器就阻塞住了，后续等待的协程得不到运行，整个程序此时也将死翘翘了。
5. CPU**只认识线程**，不会像线程一样把**上下文保存在CPU寄存器**，协程是用户控制的。
6. 协程的**优缺点**
   1. **优点**
      - 无需线程上下文切换的开销，用yield的时候，只是在函数之间来回切换
      - 无需原子操作锁定及同步的开销，**没有异步锁**之类的东西，因为协程就是单线程
      - 方便切换控制流，简化编程模型
      - 高并发-高扩展-低成本，一个CPU支持**上万个协程**都不成问题
   2. **缺点**
      - 由于是单线程的无法利用多核资源，协程本质上是单线程
      - 协程需要和进程配合才能运行在多CPU上
      - 协程阻塞时会阻塞整个程序

## 小结

### 1. 协程

1. **概念**：多个任务在一条线程上来回切换
2. **目的**：
   - 在一条线程上最大限度提高CPU的使用率
   - 在一个任务中遇到IO的时候就切换到其他任务
3. **特点**：开销很小，是用户级别(从用户级别能够感知的IO操作)，不能利用多核，数据共享，**协程之间数据安全**
4. **模块**
   1. gevent：基于greenlet切换
      - 导入模块
      - 导入monkey，执行patch_all
      - 写一个函数但做协程要执行的任务
      - 协程对象 = gevent.spawn(函数名, 参数，)
      - 协程对象.join(), gevent.joinall([g1, g2…])
   2. 分辨gevent是否识别了我们写的代码中的io操作
      - patch_all：queue.get()
      - print()：在**patch_all前后打印**io操作的函数地址
   3. asyncio：**基于yield切换**
      - **async**：标识一个协程函数
      - **await**：后面跟着一个asyncio模块提供的io操作的函数
      - **loop**：事件循环，负责在多个任务之间进行切换

### 2. 进程和线程

1. 什么是GIL
   1. 全局解释器锁
   2. 由Cpython解释器提供的
   3. 导致了一个进程中的多个线程同一时刻只能有一个线程访问CPU
2. 进程、线程中都需要用到锁
   1. **互斥锁**：在一个线程中不能连续acquire多次，效率高，产生死锁的几率大
   2. **递归锁**：在一个线程中能连续acquire多次，效率低，一把锁永远不死锁
3. 进程、线程、协程特点（**开销**，**数据隔离/共享**，**能不能利用多核**，**数据安全**，**用户还是os**）
   1. 进程：开销大，数据隔离，可以利用多核，数据不安全，os控制
   2. 线程：开销中，数据共享，cpython解释器中不能利用多核，数据不安全，os控制
   3. 协程：开销小，数据共享，不能利用多核，数据安全，用户控制
4. 在哪些地方用到了线程和协程
   1. 自己用线程、协程完成爬虫任务
   2. 但是，后来有了比较丰富的爬虫框架
      - 了解到爬虫**scrapy**、**beautyful soup**、**aiohttp**爬虫框架，哪些用到了线程、哪些用到了协程
   3. web框架中，并发是如何实现的
      - 传统框架：**django**线程实现，flask(优先选择协程、其次使用线程)
      - socket server：多线程实现
      - 异步框架：**tornado**，**sanic**都是协程实现
5. IPC
   1. 进程间通信机制
   2. 内置模块(基于文件)：queue，pipe
   3. 第三方工具(基于网络)：redis，kafka，memcache，rabbitmq
   4. 第三方：发挥的都是消息中间件的功能
6. 线程相关：
   1. 开启线程时间很短，satrt是一个异步非阻塞方法
   2. 同步数据安全
   3. 列表的操作，无论是同步还是异步都是数据安全的